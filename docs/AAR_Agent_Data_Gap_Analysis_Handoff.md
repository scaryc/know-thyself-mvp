# AAR Agent Data Gap Analysis - Handoff Document

**Date:** December 23, 2024
**Session Duration:** Full investigation and solution design
**Status:** Investigation complete, solutions designed, ready for implementation
**Next Owner:** Continue with implementation decisions

---

## Executive Summary

The AAR (After Action Review) agent currently provides limited feedback to students because it lacks two critical pieces of data:
1. **Checklist performance data** - what specific clinical actions were completed/missed and their timing
2. **Dialogue context** - the conversational flow that reveals student reasoning, hesitation, and error correction

We investigated the data flow, identified where it breaks, and designed multiple solution approaches aligned with Strategy B architecture.

**Key Insight:** The solution isn't about building new tracking - it's about **connecting existing systems** and **preserving context** that's currently being lost.

---

## Understanding Structured Memory (Strategy B Foundation)

Before diving into problems, it's critical to understand that **structured memory is already working and tracking actions during scenarios**.

### What is Structured Memory?

Strategy B's core innovation for managing context overflow. Instead of keeping verbose conversation messages (which grow unbounded), it stores events as **compact structured data**.

**Location:** `session.structuredMemory` object

**Created:** At scenario start ([index.js:2386-2397](../server/index.js#L2386-L2397))

**Populated:** Throughout scenario via helper functions:
- `addMemoryAction()` - when actions detected ([index.js:2403-2416](../server/index.js#L2403-L2416))
- `addMemoryVitals()` - when vitals measured ([index.js:2421-2434](../server/index.js#L2421-L2434))
- `addMemoryDisclosure()` - when patient reveals info ([index.js:2439-2456](../server/index.js#L2439-L2456))
- `addMemoryStateChange()` - when patient state transitions ([index.js:2458-2473](../server/index.js#L2458-L2473))

### Current Structure (What's Being Tracked):

```javascript
session.structuredMemory = {
  criticalActions: [
    {
      time: 8,  // minutes since scenario start
      action: 'oxygen_applied',
      details: { flow_rate: '15L/min', device: 'non-rebreather' },
      result: 'SpO2 88‚Üí92%'
    }
    // More actions...
  ],

  vitalsMeasurements: [
    { time: 0, HR: 128, RR: 32, SpO2: 88 },
    { time: 3, HR: 120, RR: 28, SpO2: 92 }
  ],

  patientDisclosures: [
    { time: 2, category: 'allergies', info: 'Penicillin - anaphylaxis' },
    { time: 2, category: 'medications', info: 'Albuterol daily' }
  ],

  stateHistory: [
    { time: 0, state: 'initial', reason: 'scenario_start' },
    { time: 5, state: 'deteriorating', reason: 'oxygen_delayed' }
  ],

  keyQuotes: [
    { time: 1, speaker: 'patient', quote: "I can't breathe!", emotion: 'panicked' }
  ]
};
```

### Why Structured Memory?

**Strategy B solves context overflow by:**
1. **Pruning conversation history** to last 8-10 messages (keeps token usage bounded)
2. **Storing structured events** instead of verbose messages (90% token reduction per event)
3. **Selective recall** - only includes relevant memory parts in prompts based on current action

**Trade-off:** Structured memory is efficient BUT conversation history is lost after pruning.

### The Critical Gap:

Structured memory tracks **WHAT** happened (action, time, result) but NOT:
- **Which checklist item** was completed (no mapping)
- **Whether on-time** vs target (no comparison)
- **Points earned** (no scoring)
- **Student's reasoning** (no dialogue context)
- **Hesitation or confidence** (no conversation nuance)

**This is what we need to fix for AAR to provide meaningful feedback.**

---

## Problems Identified

### Problem 1: AAR Agent Lacks Performance Detail ‚ö†Ô∏è REQUIRES DECISION

**Issue:**
AAR agent said: "Po d√¥kladnej anal√Ωze dostupn√Ωch d√°t mus√≠m bohu≈æiaƒæ kon≈°tatova≈•, ≈æe pre tento scen√°r m√°me minim√°lne inform√°ci√≠." (After thorough analysis, I must state that we have minimal information for this scenario.)

**What AAR Agent Currently Has:**
- ‚úÖ CDP evaluations (decision quality scores)
- ‚úÖ Medication errors
- ‚úÖ Patient state transitions
- ‚úÖ Overall scores
- ‚úÖ Cross-scenario patterns (6 working patterns)

**What AAR Agent is Missing:**
- ‚ùå **Checklist results** - which specific actions completed/missed, timing against targets
- ‚ùå **Dialogue context** - student reasoning, hesitation, questions, corrections

**Impact:**
AAR can only give generic feedback like "patient deteriorated" but cannot explain:
- Which specific action was missed (e.g., "You missed C-spine stabilization")
- How late actions were (e.g., "Oxygen given at 8min, target was 2min")
- Why student delayed (e.g., "You asked 'which oxygen device?' showing device selection uncertainty")
- What student did well (e.g., "You corrected from nasal cannula to NRB mask")

---

## Investigation Results

### Finding 1: Three Disconnected Tracking Systems

We discovered **three parallel tracking systems** that don't communicate with each other:

#### System A: ScenarioEngine.performanceTracker (Dormant, Has All Logic)

**Location:** `server/services/performanceTracker.js`

**Capabilities (fully implemented):**
```javascript
generateAARReport() {
  return {
    completed_actions: [
      { checklistId: "CA3", action: "Apply oxygen",
        minutesMark: 8, onTime: false,  // vs 2min target
        pointsEarned: 7, points_possible: 15 }
    ],
    missed_actions: [
      { id: "CA2", action: "C-spine stabilization",
        points_lost: 15, importance: "critical" }
    ],
    category_performance: {
      assessment: { earned: 15, total: 25, percentage: 60 },
      treatment: { earned: 40, total: 50, percentage: 80 }
    }
  }
}
```

**Has everything we need:**
- ‚úÖ Checklist matching logic
- ‚úÖ Timing comparison against targets
- ‚úÖ Scoring (on-time vs late, points earned)
- ‚úÖ Category breakdown
- ‚úÖ Gap analysis (what was missed)

**Why It's Not Working:**
- Lives in `session.engine.performanceTracker`
- Only receives data via `engine.processStudentAction(action)`
- Frontend never calls `/process-action` endpoint
- **Result: PerformanceTracker receives ZERO actions, generates empty reports**

#### System B: session.criticalActionsLog[] (Legacy, Partially Used)

**Location:** Throughout `server/index.js`

**What it tracks:**
```javascript
session.criticalActionsLog.push({
  action: 'oxygen_applied',
  timestamp: 1234567890,
  timeSinceStart: 480,  // 8 minutes
  category: 'treatment'
});
```

**Populated at:**
- Line 1112: When vitals measured
- Line 1601: When CDP evaluated
- Line 1653: During medication administration

**Goes to AAR:** ‚úÖ Yes - included in performance snapshot

**Problem:** Only stores `{action, timestamp, category}` - **NO checklist mapping, NO scoring, NO timing analysis**

#### System C: session.structuredMemory.criticalActions[] (Strategy B, Active)

**Location:** Strategy B implementation

**What it tracks:**
```javascript
session.structuredMemory.criticalActions.push({
  time: 8,  // minutes
  action: 'oxygen_applied',
  details: { flow_rate: '15L/min', device: 'non-rebreather' },
  result: 'SpO2 88‚Üí92%'
});
```

**Populated:** Real-time during scenarios via `addMemoryAction()`

**Used for:**
- ‚úÖ Core Agent context (selective recall)
- ‚úÖ Efficient token management

**Goes to AAR:** ‚ùå No - only used during scenario runtime, not saved to performance snapshot

**Problem:** Same as System B - **NO checklist mapping, NO performance analysis**

### The Disconnect:

```
Student performs action
    ‚Üì
Core Agent detects it
    ‚Üì
    ‚îú‚îÄ‚Üí System B: criticalActionsLog[] ‚Üê Basic tracking
    ‚îú‚îÄ‚Üí System C: structuredMemory[]   ‚Üê Rich context, but not saved
    ‚îî‚îÄX System A: performanceTracker   ‚Üê Has all logic, but never called

At scenario end:
    ‚îî‚îÄ‚Üí AAR gets: criticalActionsLog (basic data only)
        AAR missing: Checklist mapping, scoring, analysis
```

**Conclusion:** The logic exists (System A), the data is tracked (Systems B & C), but they never connect.

---

### Finding 2: Scenario Blueprints Have Rich Checklists

**Location:** `scenarios/en/*.json`

**Example from TBI scenario:**
```json
{
  "id": "CA2",
  "action": "Immediate C-spine protection applied",
  "category": "treatment",
  "time_target_minutes": 1,
  "points": 15,
  "importance": "critical",
  "criteria": "Manual C-spine stabilization within first minute..."
}
```

Each scenario has 8-10 checklist items with:
- Target timing
- Point values
- Importance levels
- Detailed criteria

**These are never matched against student actions.**

---

### Finding 3: Strategy B Already Tracks Actions

**Location:** `server/index.js` (Strategy B implementation)

**Current tracking:**
```javascript
session.structuredMemory.criticalActions.push({
  time: 8,
  action: 'oxygen_applied',
  details: { flow_rate: '15L/min', device: 'non-rebreather' },
  result: 'SpO2 88‚Üí92%'
});
```

**What's missing:**
```javascript
// Need to add:
checklistId: 'CA6',
target_time: 2,
onTime: false,
category: 'treatment',
points_earned: 7,
points_possible: 15
```

---

### Finding 4: Pattern Analysis Service Fully Working

**Location:** `server/services/patternAnalysisService.js`

**6 Patterns Already Working:**
1. Assessment-to-Treatment Gap - delays between recognition and action
2. Consistent Strengths Domain - which clinical areas student excels in
3. Consistent Weaknesses Domain - recurring problem areas
4. Systematic Assessment - whether follows ABC approach
5. Medication Error Type - recurring safety issues
6. Deterioration Prevention - proactive vs reactive care pattern

**19 Patterns Designed But Not Implemented:**
- High-stakes performance, error recovery, reassessment frequency, etc.

**Data Sources:**
- Uses `cdpEvaluations`, `criticalActionsLog`, `stateHistory`, `errors`
- Provides cross-scenario behavioral analysis
- Gives AAR agent "WHY" and "TREND" insights

---

### Finding 5: Conversation History Pruned - Context Lost Forever

**Critical Discovery:**

Strategy B **permanently deletes** conversation history during scenarios to manage token usage. This is intentional and necessary for efficiency, but has major implications for AAR.

**How Pruning Works:**

From Strategy B document ([lines 629-647](Strategy_B_Context_Overflow_Solution.md#L629-L647)):
```javascript
function getRecentMessageCount(session) {
  const elapsedMinutes = Math.floor((Date.now() - session.scenarioStartTime) / 60000);
  return elapsedMinutes >= 10 ? 10 : 8;  // 5 or 4 exchanges
}

function getRecentMessages(session) {
  const messageCount = getRecentMessageCount(session);
  const messages = session.messages || [];
  return messages.length <= messageCount
    ? messages
    : messages.slice(-messageCount);  // ‚Üê OLD MESSAGES DISCARDED
}
```

**What Gets Deleted:**

```
During 15-minute TBI scenario:

T+0-2min:   4 exchanges ‚Üí stored in session.messages
T+2-4min:   4 exchanges ‚Üí stored (8 total)
T+4-6min:   4 exchanges ‚Üí stored (12 total)
            ‚Üì
T+6min:     Pruning triggered!
            Exchanges 1-4 DELETED from session.messages
            Only last 8 messages kept

T+6-8min:   4 exchanges ‚Üí stored (12 total again)
T+8min:     Pruning again!
            Exchanges 5-8 DELETED

... continues pruning every 2 minutes

T+15min:    Scenario ends
            session.messages contains ONLY exchanges 11-15
            Exchanges 1-10 are GONE FOREVER
```

**Impact on AAR:**

At AAR start, `session.messages` contains only the **last 8-10 messages** of each scenario.

**What's Lost:**
- ‚ùå Early diagnostic reasoning: "What's the normal SpO2?"
- ‚ùå Initial hesitation: "Should I give oxygen?"
- ‚ùå First error: "Applying nasal cannula at 2L"
- ‚ùå Equipment uncertainty: "I'm not sure which device..."
- ‚ùå Pattern of questions showing knowledge gaps
- ‚ùå Student's learning progression during scenario

**What Remains:**
- ‚úÖ Only final exchanges (often just concluding actions)
- ‚úÖ Structured memory (but lacks conversation context)
- ‚úÖ Action timestamps (but not the reasoning behind them)

**Why This Matters for AAR:**

Example conversation showing what AAR CANNOT see:

```
[T+1] Student: "Check vitals"  ‚Üê DELETED
[T+1] AI: "*HR 128, RR 32, SpO2 88%*"  ‚Üê DELETED

[T+2] Student: "What's normal SpO2?"  ‚Üê DELETED - Shows knowledge gap
[T+2] AI: "*95-100%. This 88% is severe hypoxia*"  ‚Üê DELETED

[T+4] Student: "Should I give oxygen?"  ‚Üê DELETED - Shows hesitation
[T+4] AI: "*Yes, immediately*"  ‚Üê DELETED

[T+6] Student: "Which device?"  ‚Üê DELETED - Shows uncertainty
[T+6] AI: "*Non-rebreather at 15L for severe hypoxia*"  ‚Üê DELETED

[T+8] Student: "Applying nasal cannula 2L"  ‚Üê DELETED - First error
[T+8] AI: "*Insufficient, SpO2 only 89%*"  ‚Üê DELETED

[T+9] Student: "Oh! Switching to NRB at 15L"  ‚Üê DELETED - Self-correction
[T+10] AI: "*Good, SpO2 now 92%*"  ‚Üê DELETED

[T+13] Student: "Reassess vitals"  ‚Üê KEPT
[T+14] AI: "*Patient stable*"  ‚Üê KEPT
```

**AAR sees only:** "Student applied oxygen at some point, patient stable at end"

**AAR CANNOT see:** The 6-minute hesitation, knowledge gap about SpO2 norms, device selection error, self-correction

**This is why AAR needs either:**
1. **Conversation transcript saved** before pruning, OR
2. **Rich context captured** in structured memory at action time, OR
3. **Observer agent** watching and taking notes during scenario

---

## The User's Core Concern

**Direct quote from testing session:**

> "My fear is that even with this data set, AAR agent will not be able to analyze user work during scenario, because it will lack the context of dialogue responses during scenarios."

**Why This Matters:**

Even if we add checklist mapping (solving "WHAT was done"), without dialogue context, AAR cannot explain:
- **WHY** student delayed actions (knowledge gap? uncertainty? equipment issues?)
- **HOW** student reasoned through decisions (systematic? chaotic?)
- **WHAT** student asked about (reveals knowledge gaps)
- **WHETHER** student self-corrected errors (shows clinical judgment quality)
- **HOW** student reacted to patient deterioration (confidence? panic?)

**Example Impact:**

**With checklist data only:**
> "You applied oxygen at 8 minutes, target was 2 minutes. This 6-minute delay contributed to deterioration."

**With checklist + dialogue context:**
> "I can see you correctly identified hypoxia at minute 1 when you asked 'What's normal SpO2?' - good diagnostic thinking. However, you then spent 6 minutes uncertain about which oxygen device to use. You chose nasal cannula at 2L, saw it was insufficient, and corrected to non-rebreather at 15L. The patient then improved.
>
> The learning point: In severe hypoxia (SpO2 <90%), always start with high-flow. Your self-correction was good clinical judgment, but that initial 6-minute hesitation about device selection cost valuable time. Let's discuss how to recognize 'high-flow situations' faster."

**The first is factual but generic. The second is personalized, specific, and actionable.**

---

## Solution Options Designed

### Issue A: Checklist Performance Data

#### **Option 1: Lightweight Checklist Matcher** ‚≠ê RECOMMENDED

**Approach:** Enhance existing `structuredMemory` with checklist mapping at action time.

**Implementation:**
```javascript
function addMemoryAction(session, action) {
  const elapsedMinutes = getElapsedMinutes(session);

  // NEW: Match to checklist
  const checklistMatch = findChecklistMatch(
    session.scenario.critical_actions_checklist,
    action
  );

  session.structuredMemory.criticalActions.push({
    time: elapsedMinutes,
    ...action,
    // NEW: Checklist metadata
    checklistId: checklistMatch?.id || null,
    category: checklistMatch?.category || 'uncategorized',
    target_time: checklistMatch?.time_target_minutes || null,
    onTime: checklistMatch ? (elapsedMinutes <= checklistMatch.time_target_minutes) : null,
    points_earned: calculatePoints(checklistMatch, elapsedMinutes),
    importance: checklistMatch?.importance || null
  });
}
```

**At scenario end:**
```javascript
function generateChecklistSummary(session) {
  const completed = session.structuredMemory.criticalActions
    .filter(a => a.checklistId !== null);

  const missed = session.scenario.critical_actions_checklist
    .filter(item => !completed.some(a => a.checklistId === item.id));

  return { completed, missed, categoryBreakdown };
}
```

**Pros:**
- Minimal code (~100 lines)
- Uses existing Strategy B infrastructure
- Real-time tracking
- Efficient (no API calls)

**Cons:**
- Requires robust keyword matching logic
- May need blueprint enhancement

**Complexity:** LOW (1-2 hours)
**Cost:** $0

**The Keyword Matching Challenge:**

Matching student actions to checklist items is non-trivial because students describe actions in many ways:

**Checklist says:** "Apply high-flow oxygen"

**Student might say:**
- "Give oxygen" ‚úì (contains "oxygen")
- "Start O2 mask" ‚úì (abbreviation)
- "Apply NRB at 15L" ‚úì (doesn't mention oxygen, but NRB = non-rebreather oxygen device)
- "Put patient on high-flow" ‚úì (oxygen implied)
- "15 liters via mask" ‚úì (no oxygen keyword at all)

**Why This Happens:**
1. **Medical abbreviations:** O2, NRB, NC, BVM
2. **Device names imply action:** "non-rebreather" means oxygen
3. **Implicit actions:** "high-flow therapy" means oxygen
4. **Vocabulary variation:** "apply" = "give" = "start" = "administer"
5. **Negations:** "Don't give oxygen yet" contains "oxygen" but ISN'T a match

**Three Levels of Solution:**

1. **Simple Keywords (70% accuracy, 1 hr/scenario):**
   ```javascript
   if (action.includes('oxygen') || action.includes('o2')) {
     return checklistItem_CA6;  // Oxygen administration
   }
   ```
   - Fast to implement
   - Misses abbreviations, implicit actions
   - False positives from negations

2. **Tiered Enhancement (95% on critical, 4 hrs total):** ‚≠ê **RECOMMENDED**
   - Enhance ONLY critical checklist items (5-6 per scenario)
   - Add keywords, synonyms, device names, tool mappings
   - Other items: best-effort matching
   - Focuses effort where it matters most (safety-critical actions)

3. **Full Enhancement (95% overall, 10 hrs):**
   - All checklist items get rich metadata
   - Comprehensive synonym lists
   - Tool call mappings
   - Exclusion keywords (negations)
   - Highest accuracy but significant upfront effort

**Blueprint Enhancement Example:**
```json
{
  "id": "CA6",
  "action": "Oxygen administration",
  "matching": {
    "keywords": ["oxygen", "o2"],
    "synonyms": ["non-rebreather", "nrb", "nasal cannula", "high-flow"],
    "tool_mappings": ["administer_oxygen"],
    "exclusion_keywords": ["don't", "not yet"]
  }
}
```

---

#### **Option 2: Post-Scenario Analysis Layer**

**Approach:** Keep tracking simple, analyze at scenario end.

**Pros:**
- Doesn't slow runtime
- Can use sophisticated matching (fuzzy logic, LLM)

**Cons:**
- Analysis only after scenario
- More complex matching needed

**Complexity:** MEDIUM (3-4 hours)
**Cost:** $0

---

### Issue B: Dialogue Context

#### **Option 1: Save Full Conversation Transcript** ‚≠ê RECOMMENDED AS MINIMUM

**Core Idea:** Preserve conversation before Strategy B pruning destroys it.

**Approach:** At scenario end (in `/next-scenario` endpoint), save complete `session.messages` array to performance snapshot BEFORE clearing for next scenario.

```javascript
const performanceSnapshot = {
  scenarioId: "tbi_patient_v2_0_final",
  checklistResults: [...],
  cdpEvaluations: [...],

  // NEW: Preserve full conversation
  conversationTranscript: session.messages.map(m => ({
    role: m.role,
    content: m.content,
    timestamp: m.timestamp
  }))
};
```

**Usage at AAR:**

**NOT sent in initial AAR context** (would conflict with Strategy B efficiency).

**Instead: Selective retrieval when needed**

Example flow:
```
Student: "Why did patient deteriorate in TBI scenario?"
    ‚Üì
AAR analyzes question ‚Üí needs TBI deterioration timeline
    ‚Üì
Retrieves TBI transcript excerpt (minutes 4-10)
    ‚Üì
Includes relevant exchanges in context for answering
    ‚Üì
Responds with specific examples from conversation
```

**Retrieval Strategy Options:**

**Option A: Manual retrieval** (simple, AAR agent requests when needed)
- AAR identifies it needs conversation context
- Makes specific request to retrieve transcript excerpt
- Only pays for tokens when actually used

**Option B: Keyword-triggered** (automatic, based on question patterns)
- Questions about "why", "what happened", "how did I" trigger retrieval
- Questions about "what did I miss", "score" use structured data only
- Balances cost vs. context

**Option C: Always include summary** (moderate cost)
- Generate compact summary of conversation (200-300 tokens)
- Include in AAR context automatically
- Full transcript available for deep dives

**Pros:**
- Simple to implement (30 min)
- Preserves all context
- Zero runtime cost

**Cons:**
- 2-4k tokens per scenario stored
- Need smart retrieval logic for AAR

**Complexity:** LOW (30 minutes)
**Storage Cost:** Negligible
**AAR Usage Cost:** $0.01-0.03 per selective retrieval

---

#### **Option 2: Enhanced Structured Memory**

**Core Idea:** Since structured memory is already tracking actions in real-time, enhance it to capture surrounding conversation context at the moment actions occur.

**Approach:** Modify `addMemoryAction()` to extract context from recent messages.

```javascript
session.structuredMemory.criticalActions.push({
  time: 8,
  action: "oxygen_applied",

  // NEW: Rich context
  studentQuestion: "Should I give oxygen?",
  studentHesitation: ["not sure", "maybe"],
  aiGuidance: "Use non-rebreather for severe hypoxia",
  deviceUsed: "nasal cannula 2L",
  errorCorrected: true
});
```

**Context Extraction Logic:**

```javascript
function extractStudentContext(recentMessages) {
  const studentMessages = recentMessages.filter(m => m.role === 'user');
  const lastStudentMsg = studentMessages[studentMessages.length - 1]?.content || '';

  return {
    lastQuestion: extractQuestions(lastStudentMsg),  // "Should I give oxygen?"
    hesitationMarkers: detectHesitation(lastStudentMsg),  // ["not sure", "maybe"]
    confidenceLevel: assessConfidence(lastStudentMsg),  // HIGH/MEDIUM/LOW
    errorLanguage: detectErrorAwareness(lastStudentMsg)  // "Oh no", "I was wrong"
  };
}
```

**Pros:**
- ‚úÖ Structured, compact data (not verbose transcript)
- ‚úÖ Aligns perfectly with Strategy B philosophy
- ‚úÖ Captured in real-time as actions happen
- ‚úÖ Moderate token cost (~100 per action √ó 10 actions = 1,000 per scenario)
- ‚úÖ Goes into performance snapshot automatically
- ‚úÖ Available to AAR without special retrieval logic

**Cons:**
- ‚ö†Ô∏è Need robust context extraction logic (heuristics or simple LLM)
- ‚ö†Ô∏è Might miss subtle nuance (not full conversation)
- ‚ö†Ô∏è Limited to context around action time (misses earlier reasoning)

**Example Output:**
```javascript
{
  time: 8,
  action: "oxygen_applied",
  checklistId: "CA6",

  // Context captured:
  studentQuestion: "Which device should I use?",
  hesitationMarkers: ["not sure", "maybe"],
  confidenceLevel: "LOW",
  deviceUsed: "nasal cannula 2L",
  aiGuidance: "Use non-rebreather for severe hypoxia",
  patientResponse: "SpO2 88‚Üí89% (minimal improvement)",
  errorCorrected: true,
  correctionTime: 9,
  correctionAction: "Switched to non-rebreather 15L"
}
```

**AAR Can Say:**
> "At minute 8, you asked 'Which device should I use?'‚Äîthis showed you recognized the need for oxygen but were uncertain about device selection. You chose nasal cannula at 2L, which only improved SpO2 to 89%. Good news: you self-corrected to non-rebreather at minute 9, and the patient improved to 92%. The learning point is recognizing 'high-flow situations' faster."

**Complexity:** MEDIUM (3-4 hours)
**Cost:** ~3,000 tokens per session (1,000 per scenario √ó 3)

---

#### **Option 3: Observer Agent** ü§ñ

**Approach:** Background agent watches each exchange, takes notes.

```javascript
async function runObserverAgent(session, userMsg, aiResponse) {
  const prompt = `
Analyze this paramedic training exchange:

Student: "${userMessage}"
AI: "${aiResponse}"

Detect:
1. What action was taken?
2. Hesitation or confidence?
3. Decision quality?
4. Patient response?
5. Errors made/corrected?

Output JSON.
  `;

  const response = await anthropic.messages.create({
    model: 'claude-3-5-haiku-20241022',
    max_tokens: 300,
    messages: [{ role: 'user', content: prompt }]
  });

  session.observerNotes.push(JSON.parse(response.content[0].text));
}
```

**How It Works in Practice:**

```javascript
// After Core Agent generates response
const aiResponse = await anthropic.messages.create({...});

// In parallel (or immediately after), run observer
runObserverAgent(session, userMessage, aiResponse.content[0].text)
  .catch(err => console.error('Observer failed:', err));
  // Failures don't break scenario - just less rich AAR data

// Continue with scenario
```

**Example Observer Output:**
```javascript
{
  time: 8,
  studentMessage: "Um... applying nasal cannula at 2 liters",
  observation: {
    actionDetected: "oxygen_applied",
    deviceUsed: "nasal_cannula_2L",
    hesitationLevel: "HIGH",
    hesitationIndicators: ["um", "6-minute delay since identifying hypoxia"],
    decisionQuality: "SUBOPTIMAL",
    reasoning: "Wrong device for SpO2 88% - requires high-flow",
    patientResponse: "Minimal improvement, SpO2 only 89%",
    errorType: "device_selection",
    clinicalReasoningGap: "Doesn't know criteria for high-flow vs low-flow oxygen",
    teachingOpportunity: "Discuss oxygen delivery devices and indications"
  }
}
```

**Pros:**
- ‚úÖ **Richest context** - AI deeply understands nuance
- ‚úÖ **Intelligent analysis** - can infer reasoning quality, knowledge gaps
- ‚úÖ **Captures everything** - hesitation, corrections, emotional state
- ‚úÖ **Separate concern** - doesn't complicate Core Agent logic
- ‚úÖ **Robust to student variation** - LLM adapts to different communication styles
- ‚úÖ **Can run async** - minimal impact on scenario performance

**Cons:**
- ‚ùå **Cost:** ~$0.002 per exchange √ó 10 exchanges √ó 3 scenarios = **$0.06 per session**
- ‚ùå **Latency:** +300-500ms per turn (mitigated by async execution)
- ‚ùå **Complexity:** Error handling, JSON parsing, retry logic
- ‚ùå **Potential inconsistency:** LLM might occasionally miss things or hallucinate
- ‚ùå **Dependency:** Adds another API call point of failure

**Cost-Benefit Analysis:**

Per 100 students:
- Cost: $6 in observer agent calls
- Benefit: Significantly better AAR feedback quality
- Alternative: Tutor's time reviewing student performance manually (~15 min/student = 25 hours √ó $30/hr = $750)

**Observer agent is 125√ó cheaper than human review while providing consistent, detailed analysis.**

**Complexity:** HIGH (1-2 days)
**Cost:** +$0.06 per session (~$6 per 100 students)

---

#### **Option 4: Hybrid Strategy**

**Approach:** Enhanced memory + saved transcript + optional observer.

**How It Works:**

1. **Real-time:** Enhanced structured memory captures key context at action time
2. **Backup:** Full transcript saved at scenario end (costs nothing, just storage)
3. **Optional:** Observer agent runs selectively (every other exchange, or for critical moments only)

**At AAR:**
- **Primary source:** Enhanced structured memory (always available, efficient)
- **When needed:** Retrieve transcript excerpts for deep context
- **Enrichment:** Observer notes add intelligence when available

**Decision Logic:**
```javascript
// AAR agent responding to student question

if (needsDetailedContext) {
  // Use enhanced memory for structured data
  const actionContext = getEnhancedMemory(scenario);

  if (needsConversationFlow) {
    // Retrieve relevant transcript excerpt
    const transcript = getTranscriptExcerpt(scenario, timeRange);
  }

  if (hasObserverNotes) {
    // Merge observer insights
    const observations = getObserverNotes(scenario);
  }

  return synthesizeResponse(actionContext, transcript, observations);
}
```

**Pros:**
- ‚úÖ **Most comprehensive solution**
- ‚úÖ **Graceful degradation** - works even if components fail
- ‚úÖ **Flexible cost** - observer usage is configurable
- ‚úÖ **Best of all worlds** - structured efficiency + conversation context + intelligent analysis

**Cons:**
- ‚ö†Ô∏è **Most complex** - three data sources to manage
- ‚ö†Ô∏è **Implementation time** - need all three components working together
- ‚ö†Ô∏è **Maintenance** - more moving parts to maintain

**Complexity:** HIGH (2-3 days)
**Cost:** Variable - $0 (structured only) to $0.06/session (with observer)

---

## Comparison Matrix

### Checklist Solutions

| Solution | Data Quality | Effort | Cost | Alignment with Strategy B |
|----------|-------------|--------|------|---------------------------|
| Lightweight Matcher | 80% | 1-2 hrs | $0 | ‚≠ê‚≠ê‚≠ê Perfect |
| Post-Analysis | 80% | 3-4 hrs | $0 | ‚≠ê‚≠ê Good |

### Dialogue Context Solutions

| Solution | Context Quality | Effort | Runtime Cost | Storage | AAR Usage Cost |
|----------|----------------|--------|--------------|---------|----------------|
| Saved Transcript | High (if retrieved) | 30 min | $0 | 6-12k tokens | $0.01-0.03/retrieval |
| Enhanced Memory | Medium | 3-4 hrs | $0 | 3k tokens | $0 |
| Observer Agent | Excellent | 1-2 days | +500ms | Minimal | +$0.06/session |
| Hybrid | Excellent | 2-3 days | Variable | 3k+ | Variable |

---

## Key Decisions Made

### Decision 1: Checklist Data Gap - PENDING YOUR DECISION
**Recommendation:** Start with Lightweight Matcher (Option 1)
- Use tiered blueprint enhancement (4 hrs)
- Enhance critical checklist items with matching metadata
- Provides 80% value with minimal effort

### Decision 2: Dialogue Context - PENDING YOUR DECISION
**Recommendation:** Two-phase approach
- **Phase 1:** Always save full transcript (30 min implementation)
- **Phase 2:** Add enhanced memory OR observer agent based on testing

---

## Key Open Questions

### Question 1: Checklist Matching Approach
**Decision needed:**
- Simple keywords (70% accuracy, 2 hrs)?
- Tiered enhancement (95% on critical items, 4 hrs)?
- Full enhancement (95% overall, 10 hrs)?

### Question 2: Dialogue Context Priority
**Decision needed:**
- Is $0.06/session acceptable for observer agent?
- Do we need perfect context or good-enough?
- Should we start simple and enhance later?

### Question 3: Blueprint Enhancement Scope
**Decision needed:**
- Enhance all 4 scenarios at once?
- Start with 1 scenario as proof-of-concept?
- Focus only on critical checklist items?

---

## Technical Context

### Strategy B Architecture (Critical to Understand)

**Document:** `docs/Strategy_B_Context_Overflow_Solution.md`

**Key Points:**
1. **Structured Memory:** Instead of verbose messages, stores events as structured data
2. **Prompt Caching:** Static content cached (90% cost savings)
3. **Message Window:** Only last 8-10 messages sent to AI (4-5 exchanges)
4. **Selective Recall:** Intelligently filters memory based on current action

**Impact on AAR:**
- Full conversation history pruned during scenario
- Only structured memory and recent messages available
- Need to preserve data before pruning for AAR

### Current Data Flow

```
Student Action
    ‚Üì
Core Agent Conversation
    ‚Üì
Tool calls detected
    ‚Üì
session.criticalActionsLog.push() ‚Üê Used for AAR
session.structuredMemory.criticalActions.push() ‚Üê Strategy B
    ‚Üì
(ScenarioEngine.performanceTracker never called)
    ‚Üì
Scenario ends ‚Üí /next-scenario endpoint
    ‚Üì
generateChecklistSummary() ‚Üê Need to implement
    ‚Üì
Save to scenarioPerformanceHistory[]
    ‚Üì
AAR starts ‚Üí receives performanceHistory
    ‚Üì
patternAnalysisService.analyzePerformancePatterns()
    ‚Üì
AAR agent gets context with patterns + checklist results
```

### Key Files

**Backend:**
- `server/index.js` - Main server, Strategy B implementation
  - Lines 2400-2470: Structured memory functions
  - Lines 3420-3450: Performance snapshot creation (WHERE TO ADD CHECKLIST)
  - Lines 3860-3915: AAR initialization
- `server/services/performanceTracker.js` - Checklist scoring logic (dormant)
- `server/services/patternAnalysisService.js` - Cross-scenario patterns (working)
- `server/services/aarService.js` - AAR context building
  - Lines 274-302: buildScenarioSummary (shows checklistResults)

**Frontend:**
- `know-thyself-frontend/src/App.tsx` - AAR initialization
- `know-thyself-frontend/src/components/conversation/ConversationPanel.tsx` - Message display

**Scenarios:**
- `scenarios/en/*.json` - Scenario blueprints with checklists

---

## Recommended Implementation Path

### **Phase 1: Minimum Viable AAR (Week 1)**

**Goal:** Get AAR to 70-80% usefulness with minimal effort.

**Tasks:**
1. Implement lightweight checklist matcher (1-2 hrs)
2. Save full conversation transcript to performance snapshot (30 min)
3. Test with one scenario

**Deliverables:**
- AAR can answer: "What did I miss?" with specific checklist items
- AAR can answer: "Was I too slow?" with timing data
- Full transcript preserved for future use

**Effort:** ~2 hours
**Cost:** $0

---

### **Phase 2: Enhanced Context (Week 2)**

**Goal:** Improve AAR quality to 90%.

**Choose ONE approach:**

**Option A: Enhanced Structured Memory (3-4 hrs)**
- Capture student questions, hesitation, corrections at action time
- Moderate token cost (~3k/session)
- Good context quality

**Option B: Observer Agent (1-2 days)**
- Background AI watches conversations
- Excellent context quality
- +$0.06 per session cost

**Recommendation:** Start with Option A, add Option B if needed.

---

### **Phase 3: Blueprint Enhancement (Ongoing)**

**Approach:** Tiered enhancement

**Week 1:**
- Enhance critical checklist items in TBI scenario (1 hr)
- Test matching accuracy

**Week 2:**
- Enhance remaining TBI items + critical items in other scenarios (2 hrs)

**Week 3:**
- Complete all scenarios based on real usage patterns (1 hr)

**Total effort:** ~4 hours spread over 3 weeks

---

## Critical Success Factors

### 1. Preserve Conversation Before Pruning
Strategy B deletes early messages. Must save to performance snapshot before pruning occurs.

### 2. Align with Strategy B Architecture
Don't add verbose data to runtime. Use structured memory approach. Save detailed data only in performance snapshots.

### 3. Start Simple, Iterate
Don't over-engineer. Get basic checklist matching working first. Add observer agent only if basic context isn't enough.

### 4. Test with Real User Conversations
The actual TBI test conversation from logs would be perfect test data. Use it to validate solutions.

---

## Next Steps (For You to Decide)

### Immediate Decisions Needed:

**Decision 1: Checklist Solution**
- [ ] Go with Lightweight Matcher?
- [ ] If yes, what level of blueprint enhancement?
  - [ ] Simple keywords only (2 hrs)?
  - [ ] Tiered enhancement (4 hrs)?
  - [ ] Full enhancement (10 hrs)?

**Decision 2: Dialogue Context**
- [ ] Save transcript (30 min) - do this regardless?
- [ ] Add enhanced memory (3-4 hrs)?
- [ ] Add observer agent (1-2 days)?
- [ ] Start with transcript, decide on enhancements after testing?

**Decision 3: Implementation Approach**
- [ ] Implement everything at once?
- [ ] Phase 1 ‚Üí Test ‚Üí Phase 2?
- [ ] Proof-of-concept with one scenario first?

---

## Questions for Next AI Session

1. **Checklist Matching:** How to implement `findChecklistMatch()` function robustly?
2. **Blueprint Enhancement:** Show example enhanced matching metadata for TBI scenario
3. **Context Extraction:** How to extract student hesitation/questions from messages?
4. **Observer Agent:** Complete implementation with error handling
5. **Integration:** How to modify `/next-scenario` endpoint to generate checklist summary?
6. **AAR Context:** How to modify `buildScenarioSummary()` to use checklist data?

---

## Critical Implementation Details

### Where to Make Changes

**1. Add checklist matching to structured memory (Option 1):**
- **File:** `server/index.js`
- **Function:** `addMemoryAction()` at line 2403
- **Add:** `findChecklistMatch()` function (new)
- **Modify:** Enhanced action object with checklist metadata

**2. Generate checklist summary at scenario end:**
- **File:** `server/index.js`
- **Function:** Add `generateChecklistSummary(session)` (new)
- **Location:** Call from `/next-scenario` endpoint around line 3440
- **Save to:** `performanceSnapshot.checklistResults`

**3. Save conversation transcript:**
- **File:** `server/index.js`
- **Location:** In `/next-scenario` endpoint, line 3430
- **Add:** `conversationTranscript: session.messages.map(...)` to performanceSnapshot

**4. Enhanced memory context (if implementing):**
- **File:** `server/index.js`
- **Function:** Modify `addMemoryAction()` at line 2403
- **Add:** Context extraction functions

**5. Observer agent (if implementing):**
- **File:** `server/index.js` or new `server/services/observerAgent.js`
- **Location:** After Core Agent response in `/message` endpoint
- **Call:** `runObserverAgent(session, userMsg, aiResponse)` async

### Data Flow After Implementation

```
Student Action
    ‚Üì
Core Agent detects
    ‚Üì
addMemoryAction() ENHANCED
    ‚Üì
session.structuredMemory.criticalActions.push({
  time: 8,
  action: 'oxygen_applied',

  // NEW: Checklist mapping
  checklistId: 'CA6',
  target_time: 2,
  onTime: false,
  points_earned: 7,
  category: 'treatment',

  // NEW: Context (if enhanced memory)
  studentQuestion: "Which device?",
  hesitationLevel: "HIGH"
})
    ‚Üì
Scenario continues...
    ‚Üì
Scenario ends ‚Üí /next-scenario
    ‚Üì
generateChecklistSummary() ‚Üí NEW FUNCTION
    ‚Üì
performanceSnapshot = {
  checklistResults: [...],  // ‚Üê NEW
  conversationTranscript: [...],  // ‚Üê NEW
  observerNotes: [...]  // ‚Üê NEW (if observer agent)
}
    ‚Üì
Save to scenarioPerformanceHistory[]
    ‚Üì
AAR starts with rich data
```

---

## Context for Next AI

**What we built today:**
- Deep analysis of three disconnected tracking systems
- Understanding of Strategy B's structured memory foundation
- Identification of conversation pruning impact on AAR
- 4 solutions for checklist data (lightweight to comprehensive)
- 4 solutions for dialogue context (saved transcript to observer agent)
- Complete implementation roadmap with file locations and line numbers

**What's working:**
- ‚úÖ Structured memory actively tracking actions, vitals, disclosures during scenarios
- ‚úÖ 6 cross-scenario patterns providing behavioral analysis
- ‚úÖ CDP evaluations tracking decision quality
- ‚úÖ State history tracking patient outcomes
- ‚úÖ Medication errors tracked
- ‚úÖ Strategy B efficiently managing context during scenarios

**What's broken:**
- ‚ùå Structured memory actions never mapped to scenario checklists
- ‚ùå No timing analysis against targets
- ‚ùå No scoring or gap analysis
- ‚ùå Conversation history deleted by Strategy B pruning before AAR
- ‚ùå Three tracking systems exist but don't communicate

**The disconnect:**
- System A (PerformanceTracker): Has all the logic, receives no data
- System B (criticalActionsLog): Has basic data, no analysis capability
- System C (structuredMemory): Has rich data, no checklist mapping

**Solution essence:**
Connect the systems. Map structured memory actions to checklists in real-time. Preserve conversation before pruning. Optionally enhance with observer agent.

**Philosophy:**
Keep system simple, scalable, focused on user experience. Align with Strategy B principles: structured memory, intelligent filtering, safety-first. Build on what exists rather than creating new infrastructure.

**User's core concern (verbatim):**
"My fear is that even with this data set, AAR agent will not be able to analyze user work during scenario, because it will lack the context of dialogue responses during scenarios."

**Why this matters:**
Checklist data tells WHAT and WHEN. Dialogue context tells WHY and HOW. Together they enable personalized, actionable feedback instead of generic scores.

---

## End of Handoff

**Session Status:** Investigation complete, ready for implementation
**Recommendation:** Start with Phase 1 (Lightweight Matcher + Saved Transcript)
**Estimated effort:** 2-3 hours for working solution
**Expected AAR quality improvement:** 50% ‚Üí 80%

**All analysis, code, and documentation preserved in this handoff.**
